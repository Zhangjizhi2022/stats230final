---
title: |
  | Stats 230 Final Project Report
author: |
  | David Mwakima and Jizhi Zhang
date: |
  | 03/10/2023
fontsize: 11pt
classoption:
        - twocolumn
output: 
   bookdown::pdf_document2:
    #   pandoc_args: [
    #   "-V", "classoption=twocolumn"
    # ]
      includes:
        # in_header: Preamble.tex 
      toc: false
      toc_depth: 2
      extra_dependencies: [bbold, bbm, float]
citation_package: natbib
bibliography: ["Our_bib.bib"]
biblio-style: "apalike"
link-citations: yes
urlcolor: blue
linkcolor: blue
fig_caption: yes
---

# Introduction

What is MCMC for Bayesian inference?

[David details here]

The Metropolis-Hastings algorithm developed by @Metropolis1953 is used for MCMC @Gelfand1990. 

The use of  MCMC for Large datasets presents a new research frontier.

[David details here]


In our report we consider a paper by @Maire2019 that addresses the problem of using MCMC for large datasets. This paper proposes a new methodology, which the authors call \textit{Informed Sub-Sampling MCMC} (ISS-MCMC), for doing Bayesian MCMC approximation of the posterior distribution. This is a scalable version of the Metropolis-Hastings algorithm designed for situations when $N$ is so big that to approximate the posterior distribution takes a very long time. 

# Main ideas of how it works

ISS-MCMC is "informed" because it makes use of a measure of similarity with respect to the full dataset through summary statistics. It is "sub-sampling" because it uses this measure to select a subset of the dataset that will be used by the Markov transition kernel at the $k$-th iteration of the algorithm. In this way, the Markov chain transition kernel uses only a fraction $n/N$ of the entire dataset. They show using examples that choosing $n \ll N$ can lead to significant reductions in computational run-times while still retaining the simplicity of the standard Metropolis-Hastings algorithm. In the following subsections we consider in more detail the main ideas of how this algorithm works. See section 4.

## Similarity through summary statistics

See @Fearnhead2012 [Jizhi add details here]

## Transition kernel

[Jizhi add details here]

## The Algorithm

[Copy-paste]

# Comparison with other approaches

Other similar approaches to solve the same statistical problems are @Quiroz2018 and the \textit{Confidence Sampler} in @Bardenet2017. Both of these approaches use sophisticated "control variates" to get positive unbiased estimators (based on a subset of data) for the likelihoods in the Metropolis-Hastings acceptance ratio. The authors note that these control variates are computationally intensive.

The noisy approaches due to @Korattikara2014 and @Alquier2016

@Maclaurin2015 also addresses the computational issue are independent.

Another approach which the authors compare their approach with is an approach based on continuous time Markov processes (Langevin diffusion, Zig-Zag process) in @Fearnhead2018 and @Bierkens2019. Here, the authors note that the computational hurdle involves calculation of the gradient of the log-likelihood, which may not always be unbiased. Moreover, these approaches depart significantly from the simplicity of the original discrete M-H algorithm.

# Example with logistic regression

Here we reproduce one example in their paper for the case of logistic regression. See section 6.3

[Meet to code example next week]


## Compare convergence of ISS-MCMC with M-H.

[Add after coding example]

# References




