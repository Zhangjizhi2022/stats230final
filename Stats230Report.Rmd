---
title: |
  | Stats 230 Final Project Report
author: |
  | David Mwakima and Jizhi Zhang
date: |
  | 03/10/2023
fontsize: 11pt
classoption:
        - twocolumn
output: 
   bookdown::pdf_document2:
    #   pandoc_args: [
    #   "-V", "classoption=twocolumn"
    # ]
      includes:
        # in_header: Preamble.tex 
      toc: false
      toc_depth: 2
      extra_dependencies: [bbold, bbm, float]
citation_package: natbib
bibliography: ["Our_bib.bib"]
biblio-style: "apalike"
link-citations: yes
urlcolor: blue
linkcolor: blue
fig_caption: yes
---

# Introduction

Markov Chain Monte Carlo (MCMC) is a numerical integration technique. The main motivation for this technique is that it is often not feasible both mathematically and practically to evaluate certain integrals in high dimensions. In Bayesian statistics, in particular, all inference proceeds via the posterior distribution $p(\theta| \textbf{X})$ where $\theta$ could be a vector of multiple parameters. This distribution is given by
\[
p(\theta | \textbf{X}) = \frac{1}{C} f(\textbf{X}|\theta)p(\theta)
\]
where $f(\textbf{X}|\theta)$ is our sampling model for $\textbf{X}$, $p(\theta)$ is our prior model for $\theta$ and $C = \int_{\Theta} f(\textbf{X}|\theta) p(\theta)$ is the normalizing constant. But computing C requires integration if we are going to evaluate this posterior and use it for inference. This is difficult, especially in cases where it is not available in closed analytic form. 

Since @Gelfand1990 rediscovered the Metropolis-Rosenbluth-Hastings (MRH) algorithm developed by @Metropolis1953, this difficulty has been addressed. This algorithm relies on the theory of Markov stochastic processes. Assuming certain conditions (irreducibility, positive recurrence, detailed balance) are satisfied by a homogeneous Markov chain $\{X_{n} \}$ on state space $E$, then one can show that the chain possesses a stationary distribution $\bf{\pi}$. One then appeals to the Ergodic Theorem (a dependent samples analogue of the Strong Law of Large Numbers for i.i.d samples) that guarantees that for any initial distribution:
\[
\lim_{N \to \infty}\frac{1}{N}\sum_{k = 1}^{N}f(X_{k}) = \sum_{i \in E}f(i)\pi_{i} = E_{\pi}\big[f(X)\big]
\]

Let $p(\theta | \bf{X})$ be the target distribution, where $\bf{\theta}$ is a vector of parameters. Then with proposal densities $q(\theta^{(j)} | \theta^{(i)}, \bf{X})$, the MRH acceptance ratio is:\[
a(\theta^{(j)}, \theta^{(i)}, \textbf{X}) = \text{min}\bigg\{\frac{p(\theta^{(j)} | \textbf{X})}{p(\theta^{(i)} | \textbf{X})} \frac{q(\theta^{(i)} | \theta^{(j)}, \textbf{X})}{q(\theta^{(j)} | \theta^{(i)}, \textbf{X})}, 1\bigg\}
\]

And the Metropolis-Rosenbluth-Hastings Algorithm (MRH) for approximating $E_{p(\theta | \textbf{X})}[h(\textbf{X})]$ is:

\begin{tabular}{l  l}
\hline
& Metropolis-Rosenbluth-Hastings Algorithm \\
\hline
1: & Start with some initial value $\theta = \theta^{(0)}$ \\
2: & \textbf{for} i = 0 to N \textbf{do} \\
3: & Simulate $\theta^{(i + 1)}$ with $q(\theta^{(i + 1)} | \theta^{(i)}, \textbf{X})$\\
4: & Compute $a(\theta^{(i + 1)}, \theta^{(i)}, \textbf{X})$ \\
5: & Generate $U \sim \text{Unif}(0, 1)$ \\
6: & Accept $\theta^{(i + 1)}$ if $U \leq a(\theta^{(i + 1)}, \theta^{(i)}, \textbf{X})$ \\ 
   & Otherwise $\theta^{(i + 1)} =   \theta^{(i)}$ \\
7: & \textbf{end for}\\
8: & \textbf{return} $\frac{1}{N} \sum_{i = 1}^{N}h(X_{i})$\\
\hline
\end{tabular}

However, the use of MCMC for large datasets presents a new research frontier, see @Bardenet2014 and @Bardenet2017. This is because when \textbf{X} is large $n \gg 1$ as is typically the case in genomics, spatial statistics and cosmology, evaluating the likelihood ratio appearing in the MRH acceptance ratio is computationally intensive $O(n^{3})$.
\[
\frac{p(\theta^{(j)} | \textbf{X})}{p(\theta^{(i)} | \textbf{X})} = \frac{p(\theta^{(j)})}{p(\theta^{(i)})}\frac{\prod_{k=1}^{n}f(X_{k}|\theta^{(j)})}{\prod_{k=1}^{n}f(X_{k}|\theta^{(i)})}
\]
As a consequence MCMC with the MRH algorithm cannot be considered for reasonable runtime. 

In our report we consider a paper by @Maire2019 that addresses the problem of using MCMC for large datasets. This paper proposes a new methodology, which the authors call \textit{Informed Sub-Sampling MCMC} (ISS-MCMC), for doing Bayesian MCMC approximation of the posterior distribution. This is a scalable version of the Metropolis-Hastings algorithm designed for situations when $N$ is so big that to approximate the posterior distribution takes a very long time. 

# Main ideas of how it works

ISS-MCMC is "informed" because it makes use of a measure of similarity with respect to the full dataset through summary statistics. It is "sub-sampling" because it uses this measure to select a subset of the dataset that will be used by the Markov transition kernel at the $k$-th iteration of the algorithm. In this way, the Markov chain transition kernel uses only a fraction $n/N$ of the entire dataset. They show using examples that choosing $n \ll N$ can lead to significant reductions in computational run-times while still retaining the simplicity of the standard Metropolis-Hastings algorithm. In the following subsections we consider in more detail the main ideas of how this algorithm works. See section 4.

## Similarity through summary statistics

See @Fearnhead2012 

[Jizhi add details here]

## Transition kernel

[Jizhi add details here]

## The Algorithm

[Copy-paste]

# Comparison with other approaches

Other similar approaches to solve the same statistical problems are @Quiroz2018 and the \textit{Confidence Sampler} in @Bardenet2017. Both of these approaches use sophisticated "control variates" to get positive unbiased estimators (based on a subset of data) for the likelihoods in the Metropolis-Hastings acceptance ratio. The authors note that these control variates are computationally intensive.

The noisy approaches due to @Korattikara2014 and @Alquier2016

@Maclaurin2015 also addresses the computational issue are independent.

Another approach which the authors compare their approach with is an approach based on continuous time Markov processes (Langevin diffusion, Zig-Zag process) in @Fearnhead2018 and @Bierkens2019. Here, the authors note that the computational hurdle involves calculation of the gradient of the log-likelihood, which may not always be unbiased. Moreover, these approaches depart significantly from the simplicity of the original discrete M-H algorithm.

# Example with logistic regression

Here we reproduce one example in their paper for the case of logistic regression. See section 6.3

[Meet to code example next week]


## Compare convergence of ISS-MCMC with M-H.

[Add after coding example]

# References




