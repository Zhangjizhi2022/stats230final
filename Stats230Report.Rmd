---
title: |
  | Stats 230 Final Project Report
author: |
  | David Mwakima and Jizhi Zhang
date: |
  | 03/20/2023
fontsize: 11pt
classoption:
        - twocolumn
output: 
   bookdown::pdf_document2:
    #   pandoc_args: [
    #   "-V", "classoption=twocolumn"
    # ]
      includes:
        # in_header: Preamble.tex 
      toc: false
      toc_depth: 2
      extra_dependencies: [bbold, bbm, float]
citation_package: natbib
bibliography: ["Our_bib.bib"]
biblio-style: "apalike"
link-citations: yes
urlcolor: blue
linkcolor: blue
fig_caption: yes
---

# Introduction

Markov Chain Monte Carlo (MCMC) is a numerical integration technique. The main motivation for this technique is that it is often not feasible both mathematically and practically to evaluate certain integrals in high dimensions. In Bayesian statistics, in particular, all inference proceeds via the posterior distribution $p(\theta| \textbf{X})$ where $\theta$ could be a vector of multiple parameters. This distribution is given by
\[
p(\theta | \textbf{X}) = \frac{1}{C} f(\textbf{X}|\theta)p(\theta)
\]
where $f(\textbf{X}|\theta)$ is our sampling model for $\textbf{X}$, $p(\theta)$ is our prior model for $\theta$ and $C = \int_{\Theta} f(\textbf{X}|\theta) p(\theta)$ is the normalizing constant. But computing C requires integration if we are going to evaluate this posterior and use it for inference. This is difficult, especially in cases where: (1) an expression for the integrand is not available in closed analytic form and (2) high dimensional models with 5 or more parameters.

Since @Gelfand1990 rediscovered the Metropolis-Rosenbluth-Hastings (MRH) algorithm developed by @Metropolis1953, this difficulty in Bayesian analysis has been addressed. This algorithm relies on the theory of Markov stochastic processes. Assuming certain conditions (irreducibility, positive recurrence, detailed balance) are satisfied by a homogeneous Markov chain $\{X_{n} \}$ on state space $E$, then one can show that the chain possesses a stationary distribution $\bf{\pi}$. One then appeals to the Ergodic Theorem (a dependent samples analogue of the Strong Law of Large Numbers for i.i.d samples) that guarantees that for any initial distribution:
\[
\lim_{N \to \infty}\frac{1}{N}\sum_{k = 1}^{N}f(X_{k}) = \sum_{i \in E}f(i)\pi_{i} = E_{\pi}\big[f(X)\big]
\]

Here's how the algorithm works for Bayesian inference. Let $p(\theta | \bf{X})$ be the target distribution, where $\bf{\theta}$ is a vector of parameters. Then with proposal densities $q(\theta^{(j)} | \theta^{(i)}, \bf{X})$, the MRH acceptance ratio is:\[
a(\theta^{(j)}, \theta^{(i)}, \textbf{X}) = \text{min}\bigg\{\frac{p(\theta^{(j)} | \textbf{X})}{p(\theta^{(i)} | \textbf{X})} \frac{q(\theta^{(i)} | \theta^{(j)}, \textbf{X})}{q(\theta^{(j)} | \theta^{(i)}, \textbf{X})}, 1\bigg\}
\]

And the MRH algorithm, in pseudo-code, for approximating $E_{p(\theta | \textbf{X})}[h(\textbf{X})]$ is:

\begin{tabular}{l  l}
\hline
& Metropolis-Rosenbluth-Hastings Algorithm \\
\hline
1: & Start with some initial value $\theta = \theta^{(0)}$ \\
2: & \textbf{for} i = 0 to N \textbf{do} \\
3: & Simulate $\theta^{(i + 1)}$ with $q(\theta^{(i + 1)} | \theta^{(i)}, \textbf{X})$\\
4: & Compute $a(\theta^{(i + 1)}, \theta^{(i)}, \textbf{X})$ \\
5: & Generate $U \sim \text{Unif}(0, 1)$ \\
6: & Accept $\theta^{(i + 1)}$ if $U \leq a(\theta^{(i + 1)}, \theta^{(i)}, \textbf{X})$ \\ 
   & Otherwise $\theta^{(i + 1)} =   \theta^{(i)}$ \\
7: & \textbf{end for}\\
8: & \textbf{return} $\frac{1}{N} \sum_{i = 1}^{N}h(X_{i})$\\
\hline
\end{tabular}

However, the use of MCMC for large datasets presents a new research frontier (@Bardenet2014 and @Bardenet2017). This is because when \textbf{X} is large $n \gg 1$, as is typically the case in genomics, spatial statistics and cosmology; evaluating the likelihood ratio
\[
\frac{p(\theta^{(j)} | \textbf{X})}{p(\theta^{(i)} | \textbf{X})} = \frac{p(\theta^{(j)})}{p(\theta^{(i)})}\frac{\prod_{k=1}^{n}f(X_{k}|\theta^{(j)})}{\prod_{k=1}^{n}f(X_{k}|\theta^{(i)})}
\] appearing in the MRH acceptance ratio is computationally intensive (@Bardenet2014).

As a consequence MCMC with the MRH algorithm cannot be considered for reasonable runtime when $n$ is very large. 

In our report we consider a paper by @Maire2019 that addresses the problem of using MCMC for large datasets. This paper proposes a new methodology, which the authors call \textit{Informed Sub-Sampling MCMC} (ISS-MCMC), for doing Bayesian MCMC approximation of the posterior distribution. This is a scalable version of the Metropolis-Hastings algorithm designed for situations when $n$ is so big that to approximate the posterior distribution takes a very long time. ISS-MCMC is "informed" because it makes use of a measure of similarity with respect to the full dataset through summary statistics. It is "sub-sampling" because it uses this measure to select a subset of the dataset that will be used by the Markov transition kernel at the $k$-th iteration of the algorithm. In this way, the Markov chain transition kernel uses only a fraction $n/N$ of the entire dataset. They show using examples that choosing $n \ll N$ can lead to significant reductions in computational run-times while still retaining the simplicity of the standard Metropolis-Hastings algorithm. Moreover, unlike other approaches (discussed below), their method can be applied to virtually any model (involving i.i.d. data or not) and it does not require any assumption on the likelihood function nor on the prior distribution.

# Comparison with other approaches

The authors note that sub-sampling of the full dataset strategy has been proposed elsewhere, in particular, by @Korattikara2014, @Bardenet2014 and  @Maclaurin2015. The key difference between their approach and these is that the Markov chain transition kernel only uses a fraction $n/N$ of the available data which is by construction held constant throughout the algorithm. It is the subset variable that is randomly refreshed at each iteration according to the similarity measure.

Other similar approaches to solve these big data problems are @Quiroz2018 and the \textit{Confidence Sampler} in @Bardenet2017. Both of these approaches use sophisticated "control variates" to get positive unbiased estimators (based on a subset of data) for the likelihoods in the MRH acceptance ratio. The authors note that these control variates are still computationally intensive.

The authors also find some affinity to their work in \textit{"noisy"} or \textit{"inexact"} approaches to MCMC due to @Korattikara2014 and @Alquier2016, where an approximate MRH rule based on a sequential hypothesis test is used accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. The cost here is that the number of likelihood evaluations is adaptively set by the algorithm at each iteration. When the chain reaches equilibrium, the computational complexity is of order $O(n)$. This number can be brought down if an accurate proxy of the log-likelihood ratio, acting as control variates, is available, as demonstrated in @Bardenet2017.

Another approach which the authors compare their approach with is an approach based on continuous time Markov processes (Langevin diffusion, Zig-Zag process) in @Fearnhead2018 and @Bierkens2019. Here, the authors note that the computational hurdle involves calculation of the gradient of the log-likelihood, which may not always be unbiased. Moreover, these approaches depart significantly from the simplicity of the original discrete MRH algorithm.In the following sections we consider in more detail the main ideas of how this algorithm works (Section 3) and implement one of their examples of estimating the parameters in a logistic regression model with $N = 10^{6}$ using their algorithm.

# Main ideas of how it works

Let $(Y_{1}, \dots, Y_N)$ be a set of observed data and define
$Y_{U} = \{Y_{k}, k \in U \}$ where $U \subset \{1, \dots, N\}$. Let $\mathcal{S}: Y \to S\subseteq \mathbb{R}^{s}$. So $\mathcal{S}$ takes the data and returns an $s$ dimensional function of them, $S\subseteq \mathbb{R}^{s}$. If the model admits a sufficient statistic, then $\mathcal{S}$ maps $Y$ or a subset of it $Y_{U}$ to it, otherwise $\mathcal{S}$ maps $Y$ or a subset of it $Y_{U}$ to summary statistics $\bar{S} = S(Y_{U})/n$. See @Maire2019 p. 452, 453, 456. 

For all $n \leq N$ , they define $\textsf{U}_{n}$ as the set of possible combinations of $n$ different integer numbers less than or equal to $N$ and $\mathcal{U}_{n}$ as the powerset of $\textsf{U}_{n}$. For any subset $U \in \textsf{U}_{n}$ define the vector of difference of sufficient statistics between the whole dataset and the subset $Y_{U}$ to be:

\begin{equation}\label{eqn1}
\Delta_n(U) = \sum_{k = 1}^{N}\mathcal{S}(Y_{k}) - N/n\sum_{k \in U}\mathcal{S}(Y_{k})
\end{equation}

If there is no sufficient statistic define an analogous difference measure using summary statistics as:

\begin{equation}\label{eqn2}
\bar{\Delta}_{n}(U) = S(Y)/N - S(Y_{U})/n
\end{equation}


## Similarity through summary statistics

They then consider the distribution $\nu_{n, \epsilon}$ on the discrete space $\textsf{U}_{n}$ defined for all $\epsilon \geq 0$ by:

\begin{equation}\label{eqn3}
\nu_{n, \epsilon}(U) \propto \text{exp}(-\epsilon ||\Delta_{n}(U) ||^2)
\end{equation}

The distribution $\nu_{n, \epsilon}$ assigns a weight to any subset according to its representativeness with respect to the full dataset. It is a kind of tuning parameter. When $\epsilon = 0$, $\nu_{n, \epsilon}$ is uniform on $U_{n}$, while when $\epsilon \to \infty$, $\nu_{n, \epsilon}$ is uniform on the set of subsets that minimize $||\Delta_{n}(U)||$. The authors note that moving to general models (i.e. non i.i.d and non-exponential) amounts to relaxing the sufficient statistics existence assumption as well as the $\epsilon \to \infty$ condition. This is achieved by constructing a class of summary statistics for the model at hand and for which the following result holds. (We do not go into the details here - See @Maire2019 p. 454 Proposition 3)

For any $\theta \in \Theta$ and $\epsilon \> 0$, there exists $M < \infty$ such that 
\[
\mathbb{E}_{n, \epsilon} \bigg\{\frac{f(Y | \theta)}{f(Y_{U}|\theta)^{N/n}}\bigg\} < M
\]

## Setting up the algorithm


In the Informed Sub-Sampling method, the set of good subsamples is treated as a series of missing data (denoted by $U_{1}$, $U_{2}$, and so on) and is simulated using the ISS-MCMC algorithm. This algorithm generates a Markov chain on the extended space $\theta \times  \mathcal{U}_{n}$, where $\mathcal{U}_{n}$ is the set of possible sub-samples. The sequence of sub samples is randomly updated in a way that favors those subsets with summary statistics that are similar to the full dataset, as suggested by the analysis in previous sections. The process is implemented using a symmetric transition kernel $R$ on $(\textsf{U}_n, \mathcal{U_n})$. A transition from $(\theta_i,U_i)$ to $(\theta_{I+1} , U_{i+1})$ involves two steps:

(i)

(a) Propose a new subset variable $U \sim R(U_i, \cdot)$

(b) Set $U_{i+1}=U$ with probability 
\begin{equation}\label{eqn4}
\beta(U_i, U)= \text{min}\{ b(U_i, U), 1\}
\end{equation}
, where 
\begin{equation}\label{eqn5}
b(U_i, U) = \exp \left( \epsilon(|\Delta_n(U_i)|^2 - |\Delta_n(U)|^2) \right)
\end{equation}, and $U_{i+1} = U_i$ with probability $1 - \beta(U_i, U)$. Here, $\Delta_n$ is defined in equation 2. The transition kernel R is selected based on the data and distribution.

(ii)

(a)propose a new parameter $\theta \sim Q(\theta_i, \cdot)$

(b)Set $\theta_{i+1}=\theta$ with probability 
\begin{equation}\label{eqn6}
\alpha(\theta_i, \theta)= \text{min} \{a(\theta_i, \theta), 1\}
\end{equation}
, where 
\begin{equation}\label{eqn7}
a(\theta_i, \theta) =  \frac{\pi_n(\theta \mid Y_{U_{i+1}})Q(\theta, \theta_i)} {\pi_n(\theta_i \mid Y_{U_{i+1}})Q(\theta_i, \theta)}
\end{equation}, and $\theta_{i+1} = \theta_i$ with probability $1 - \alpha(U_i, U)$. The transition kernel Q is selected based on the data and distribution.

## The Algorithm

Here is the pseudo-code for the Informed Subsampling Algorithm:

\begin{tabular}{l  l}
\hline
&Informed Sub-Sampling MCMC\\
&Algorithm \\
\hline
1: &\textbf{Input}: \textbf{initial state}\\
   &$(\tilde{\theta}_{0}, U_{0})$ \\
   &\textbf{and summary statistics} \\
   &$S_{0} = \bar{S}(Y_{U_{0}})$\\
   &$S^{*} = \bar{S}(Y)$ \\
2: &\textbf{for} $i = 1, 2, \dots$ \textbf{do} \\
3: & propose a new subset\\
   & $U \sim R(U_{i=1}, \cdot)$ \\
   & and draw \\
   & $J \sim \text{Unif}(0,1)$ \\
4: & compute $S = \bar{S}(Y_{U})$ and \\
   & $b = b(U_{i - 1}, U)$\\
   & defined in (4) \\
5: & \textbf{if} $J \leq b$ \textbf{then}\\
6: &  set $U_{i} = U$ and $S_{i} = S$\\
7: & \textbf{else}\\
8: &  set $U_{i} = U_{i -1}$\\
   &  and $S_{i} = S_{i - 1}$ \\
9: & \textbf{end if} \\
10:& propose a new parameter \\
   & $\tilde{\theta} \sim Q(\tilde{\theta}_{i - 1}; \cdot)$ \\
   & and draw $I \sim \text{Unif}(0, 1)$ \\
11:& compute $\tilde{\pi}(\tilde{\theta}_{i - 1}|Y_{U_{i}})$,\\
   & $\tilde{\pi}_{n}(\tilde{\theta}|Y_{U_{i}})$ and \\ 
   & $\tilde{a}(\tilde{\theta}_{i - 1}|U_{i})$ defined in (7)\\
12:& \textbf{if} $I \leq \tilde{a}$ \textbf{then}\\
13:& set $\tilde{\theta_{i}} = \tilde{\theta}$ \\
14:& \textbf{else} \\
15:& set $\tilde{\theta_{i}} = \tilde{\theta_{i - 1}}$\\
16:& \textbf{end if} \\
17:& \textbf{end for} \\
18:& \textbf{return: Markov Chain} $\{(\tilde{\theta_{i}}, U_{i}), i \in \mathbb{N}\}$ \\
\hline
\end{tabular}


# Example with logistic regression

Here we implement one example from their paper. Their example involves simulated logistic regression data with $N= 10^6$ observations. However given our limited computational memory, we could not implement the full $N = 10^{6}$ simulated example. Instead, we ran our simulation for $N = 10^5$ using and $n = 1000, 5000$ and $1000$ and compared our results to the MRH without sub sampling.

Consider the logistic regression model:
\begin{equation}
P(Y=1) = \frac{\exp(\beta_0 + \beta_1X_1 + \beta_2X_2)}{1+\exp(\beta_0 + \beta_1X_1 + \beta_2X_2)}
\end{equation}

We simulate N times with $\beta_0=-1$, $\beta_1=1$ and $\beta_2=0$

We define the prior as a multinormal distribution with mean $\mu=(0,0,0)^T$ and variance $\Sigma=100I$.

The transition kernel of subsets $R(U_i,U)$ is defined as a random sample from the whole set$\textsf{U}$.

The transition kernel of the parameter$Q(\beta_i,\beta)$ is defined as a uniform distribution in $\mathbb{R}^3$ with mean $\beta_i$ and width $\delta=0.2$

```{r "MCMC code", echo=FALSE, cache=TRUE}
##Simulated Data
library(MASS)
set.seed(17)
mean<-c(1,1)
sigma<-matrix(c(1,0,0,1),nrow = 2,ncol = 2)
N = 100000
sim<-mvrnorm(N,mean,sigma)
y=rbinom(N,1,exp(-1+sim[,1])/(1+exp(-1+sim[,1])))
n = 1000
d = 3
epsilon = 0.005
delta = 0.2
iter = 1000



##ISS_MCMC_logistic function

ISS_MCMC_logistic <- function(sim, y, n, d, delta, epsilon, iter)
{
  #storing subsamples and output
  U <- matrix(NA, nrow=N, ncol=iter+1)
  betas <- matrix(NA, nrow=d, ncol=iter+1)
  
  #initializing
  betas[, 1] <- c(rep(0.5, d))
  U[, 1] <- c(rep(0, N-n), rep(1, n))
  
  #likelihood function
  loglike <- function(beta, x, y) {
    eta <- x %*% beta
    loglike <- sum(y * eta - log(1 + exp(eta)))
    return(loglike)
  }
  
  # Define the prior distribution for beta
  prior <- function(beta) {
    prior <- dnorm(beta, 0, 10, log = TRUE)
    return(prior)
  }
  
  for (i in 1:iter){
    set.seed(i)
    U.prop <-sample(1:N, n, replace = FALSE)
    y.cur <- y[which(U[, i] == 1)]
    y.prop <- y[U.prop]
    delta_n.prop <- sum(y) - N/n*sum(y.prop)
    delta_n <- sum(y)- N/n*sum(y.cur)
    b <- exp(epsilon*(delta_n^2-delta_n.prop^2))
    U[,i+1] <- rep(0,N)
    if (runif(1) < b){
      U[,i+1][U.prop] <- 1
    }
    else U[,i+1] <- U[,i]
    betas.prop <- rep(0,3)
    betas.prop[1] <- betas[1,i]+runif(1,min = -delta, max = delta)
    betas.prop[2] <- betas[2,i]+runif(1,min = -delta, max = delta)
    betas.prop[3] <- betas[3,i]+runif(1,min = -delta, max = delta)
    y.cur <- y[which(U[, i+1] == 1)]
    x.cur <- cbind(1,sim[which(U[, i+1] == 1),1],sim[which(U[, i+1] == 1),2])
    log_ratio <- loglike(betas.prop, x.cur, y.cur) + prior(betas.prop) -
      loglike(betas[,i], x.cur, y.cur) - prior(betas[,i])
    ratio <- mean(exp(log_ratio))
    betas[,i+1] <- rep(0,3)
    if (runif(1) < ratio){
      betas[,i+1] <- betas.prop
    }
    else betas[,i+1] <- betas[,i]
  }
  return(list(betas=betas, U=U))
}

## MRH_MCMC_logistic function


MRH_MCMC_logistic <- function(sim,y, d, delta){
  
    #allocate space to store output
    betas <- matrix(NA, nrow=d, ncol=iter+1)
    
    #initializing
    betas[, 1] <- c(rep(0.5, d))
    
    #likelihood function
    loglike <- function(beta, x, y) {
      eta <- x %*% beta
      loglike <- sum(y * eta - log(1 + exp(eta)))
      return(loglike)
    }
    
    # Define the prior distribution for beta
    prior <- function(beta) {
      prior <- dnorm(beta, 0, 10, log = TRUE)
      return(prior)
    }
    
    #running the MRH algorithm
    for (i in 1:iter){
    betas.prop <- rep(0,3)
    betas.prop[1] <- betas[1,i]+runif(1,min = -delta, max = delta)
    betas.prop[2] <- betas[2,i]+runif(1,min = -delta, max = delta)
    betas.prop[3] <- betas[3,i]+runif(1,min = -delta, max = delta)
    y.cur <- y
    x.cur <- cbind(1,sim[,1],sim[,2])
    log_ratio <- loglike(betas.prop, x.cur, y.cur) + prior(betas.prop) -
      loglike(betas[,i], x.cur, y.cur) - prior(betas[,i])
    ratio <- mean(exp(log_ratio))
    betas[,i+1] <- rep(0,3)
    if (runif(1) < ratio){
      betas[,i+1] <- betas.prop
    } else betas[,i+1] <- betas[,i]
  }  
  return(betas)
}
```


## Compare computational time of ISS-MCMC with M-H.

Finally, we present tables that compare the computational time of using ISS-MCMC. See Table \@ref(tab:tab1).

```{r "computing time", echo=FALSE, cache=TRUE}
mrhtime <-system.time({MRH_MCMC_logistic(sim,y, d, delta)})
iss_100 <-system.time({ISS_MCMC_logistic(sim, y, n=100, d, delta, epsilon, iter)})
iss_500 <-system.time({ISS_MCMC_logistic(sim, y, n=500, d, delta, epsilon, iter)})
iss_1000<-system.time({ISS_MCMC_logistic(sim, y, n=1000, d, delta, epsilon, iter)})
```


```{r "display", echo=FALSE}
library(kableExtra)
comparison <- rbind(mrhtime, iss_100, iss_500, iss_1000)
comparison <- as.matrix(comparison[, 3])
colnames(comparison) <- c("Computational Time (s)")
rownames(comparison) <- c("MRH", "ISS MCMC (n = 100)",
                          "ISS MCMC (n = 500)",
                          "ISS MCMC (n = 1000)")
```


```{r "tab1", echo=FALSE}
comparison %>%
  kable(format="latex",
        digits=4,
        caption ="Comparing Computational Time of MRH to ISS MCMC",
        booktabs = TRUE)
```

# Conclusion and Future Work

Since we could not handle simulated data of the order $10^6$, future work can include using a computer with more memory to handle such a large dataset in order to fully reproduce their example and check for any significant gains in reducing computational time. Also future work can include using different tuning parameters $\epsilon$ to investigate the quality the estimates for different choices of $\epsilon$.

\newpage
# Appendix

Here are marginal distributions and traceplots obtained using the ISS-MCMC algorithm.

```{r "plots", echo=FALSE, cache=TRUE}
r1=MRH_MCMC_logistic(sim,y, d, delta)
r2=ISS_MCMC_logistic(sim, y, n=100, d, delta, epsilon=0.00005, iter)$betas
r3=ISS_MCMC_logistic(sim, y, n=500, d, delta, epsilon=0.00005, iter)$betas
r4=ISS_MCMC_logistic(sim, y, n=1000, d, delta, epsilon=0.00005, iter)$betas
r5=ISS_MCMC_logistic(sim, y, n=100, d, delta, epsilon=0, iter)$betas
r6=ISS_MCMC_logistic(sim, y, n=500, d, delta, epsilon=0, iter)$betas
r7=ISS_MCMC_logistic(sim, y, n=1000, d, delta, epsilon=0, iter)$betas
```


```{r "plots", echo=FALSE, cache=TRUE}
# for n=100
#beta0
plot(density(r1[1,]),xlab = 'beta1',ylab = 'density',xlim=c(-1.5,-0.5))
lines(density(r2[1,]),col='red')
lines(density(r5[1,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

#beta1
plot(density(r1[2,]),xlab = 'beta1',ylab = 'density',xlim=c(0.5,1.5))
lines(density(r2[2,]),col='red')
lines(density(r5[2,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

#beta2
plot(density(r1[2,]),xlab = 'beta2',ylab = 'density',xlim=c(-0.5,0.5))
lines(density(r2[2,]),col='red')
lines(density(r5[2,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)


# for n=500
#beta0
plot(density(r1[1,]),xlab = 'beta1',ylab = 'density',xlim=c(-1.5,-0.5))
lines(density(r3[1,]),col='red')
lines(density(r6[1,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

#beta1
plot(density(r1[2,]),xlab = 'beta1',ylab = 'density',xlim=c(0.5,1.5))
lines(density(r3[2,]),col='red')
lines(density(r6[2,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

#beta2
plot(density(r1[2,]),xlab = 'beta2',ylab = 'density',xlim=c(-0.5,0.5))
lines(density(r3[2,]),col='red')
lines(density(r6[2,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

# for n=1000
#beta0
plot(density(r1[1,]),xlab = 'beta1',ylab = 'density',xlim=c(-1.5,-0.5))
lines(density(r4[1,]),col='red')
lines(density(r7[1,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

#beta1
plot(density(r1[2,]),xlab = 'beta1',ylab = 'density',xlim=c(0.5,1.5))
lines(density(r4[2,]),col='red')
lines(density(r7[2,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

#beta2
plot(density(r1[2,]),xlab = 'beta2',ylab = 'density',xlim=c(-0.5,0.5))
lines(density(r4[2,]),col='red')
lines(density(r7[2,]),col='blue')
legend("topright", legend = c("MH", "ISS epsilon=0.00005", "ISS epsilon=0"), 
       col = c("black", "red", "blue"), lwd = 2)

```


\newpage
# References



