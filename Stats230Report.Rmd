---
title: |
  | Stats 230 Final Project Report
author: |
  | David Mwakima and Jizhi Zhang
date: |
  | 03/20/2023
fontsize: 11pt
classoption:
        - twocolumn
output: 
   bookdown::pdf_document2:
    #   pandoc_args: [
    #   "-V", "classoption=twocolumn"
    # ]
      includes:
        # in_header: Preamble.tex 
      toc: false
      toc_depth: 2
      extra_dependencies: [bbold, bbm, float]
citation_package: natbib
bibliography: ["Our_bib.bib"]
biblio-style: "apalike"
link-citations: yes
urlcolor: blue
linkcolor: blue
fig_caption: yes
---

# Introduction

Markov Chain Monte Carlo (MCMC) is a numerical integration technique. The main motivation for this technique is that it is often not feasible both mathematically and practically to evaluate certain integrals in high dimensions. In Bayesian statistics, in particular, all inference proceeds via the posterior distribution $p(\theta| \textbf{X})$ where $\theta$ could be a vector of multiple parameters. This distribution is given by
\[
p(\theta | \textbf{X}) = \frac{1}{C} f(\textbf{X}|\theta)p(\theta)
\]
where $f(\textbf{X}|\theta)$ is our sampling model for $\textbf{X}$, $p(\theta)$ is our prior model for $\theta$ and $C = \int_{\Theta} f(\textbf{X}|\theta) p(\theta)$ is the normalizing constant. But computing C requires integration if we are going to evaluate this posterior and use it for inference. This is difficult, especially in cases where: (1) an expression for the integrand is not available in closed analytic form and (2) high dimensional models with 5 or more parameters.

Since @Gelfand1990 rediscovered the Metropolis-Rosenbluth-Hastings (MRH) algorithm developed by @Metropolis1953, this difficulty in Bayesian analysis has been addressed. This algorithm relies on the theory of Markov stochastic processes. Assuming certain conditions (irreducibility, positive recurrence, detailed balance) are satisfied by a homogeneous Markov chain $\{X_{n} \}$ on state space $E$, then one can show that the chain possesses a stationary distribution $\bf{\pi}$. One then appeals to the Ergodic Theorem (a dependent samples analogue of the Strong Law of Large Numbers for i.i.d samples) that guarantees that for any initial distribution:
\[
\lim_{N \to \infty}\frac{1}{N}\sum_{k = 1}^{N}f(X_{k}) = \sum_{i \in E}f(i)\pi_{i} = E_{\pi}\big[f(X)\big]
\]

Here's how the algorithm works for Bayesian inference. Let $p(\theta | \bf{X})$ be the target distribution, where $\bf{\theta}$ is a vector of parameters. Then with proposal densities $q(\theta^{(j)} | \theta^{(i)}, \bf{X})$, the MRH acceptance ratio is:\[
a(\theta^{(j)}, \theta^{(i)}, \textbf{X}) = \text{min}\bigg\{\frac{p(\theta^{(j)} | \textbf{X})}{p(\theta^{(i)} | \textbf{X})} \frac{q(\theta^{(i)} | \theta^{(j)}, \textbf{X})}{q(\theta^{(j)} | \theta^{(i)}, \textbf{X})}, 1\bigg\}
\]

And the MRH algorithm, in pseudo-code, for approximating $E_{p(\theta | \textbf{X})}[h(\textbf{X})]$ is:

\begin{tabular}{l  l}
\hline
& Metropolis-Rosenbluth-Hastings Algorithm \\
\hline
1: & Start with some initial value $\theta = \theta^{(0)}$ \\
2: & \textbf{for} i = 0 to N \textbf{do} \\
3: & Simulate $\theta^{(i + 1)}$ with $q(\theta^{(i + 1)} | \theta^{(i)}, \textbf{X})$\\
4: & Compute $a(\theta^{(i + 1)}, \theta^{(i)}, \textbf{X})$ \\
5: & Generate $U \sim \text{Unif}(0, 1)$ \\
6: & Accept $\theta^{(i + 1)}$ if $U \leq a(\theta^{(i + 1)}, \theta^{(i)}, \textbf{X})$ \\ 
   & Otherwise $\theta^{(i + 1)} =   \theta^{(i)}$ \\
7: & \textbf{end for}\\
8: & \textbf{return} $\frac{1}{N} \sum_{i = 1}^{N}h(X_{i})$\\
\hline
\end{tabular}

However, the use of MCMC for large datasets presents a new research frontier (@Bardenet2014 and @Bardenet2017). This is because when \textbf{X} is large $n \gg 1$, as is typically the case in genomics, spatial statistics and cosmology; evaluating the likelihood ratio
\[
\frac{p(\theta^{(j)} | \textbf{X})}{p(\theta^{(i)} | \textbf{X})} = \frac{p(\theta^{(j)})}{p(\theta^{(i)})}\frac{\prod_{k=1}^{n}f(X_{k}|\theta^{(j)})}{\prod_{k=1}^{n}f(X_{k}|\theta^{(i)})}
\] appearing in the MRH acceptance ratio is computationally intensive (@Bardenet2014).

As a consequence MCMC with the MRH algorithm cannot be considered for reasonable runtime when $n$ is very large. 

In our report we consider a paper by @Maire2019 that addresses the problem of using MCMC for large datasets. This paper proposes a new methodology, which the authors call \textit{Informed Sub-Sampling MCMC} (ISS-MCMC), for doing Bayesian MCMC approximation of the posterior distribution. This is a scalable version of the Metropolis-Hastings algorithm designed for situations when $n$ is so big that to approximate the posterior distribution takes a very long time. ISS-MCMC is "informed" because it makes use of a measure of similarity with respect to the full dataset through summary statistics. It is "sub-sampling" because it uses this measure to select a subset of the dataset that will be used by the Markov transition kernel at the $k$-th iteration of the algorithm. In this way, the Markov chain transition kernel uses only a fraction $n/N$ of the entire dataset. They show using examples that choosing $n \ll N$ can lead to significant reductions in computational run-times while still retaining the simplicity of the standard Metropolis-Hastings algorithm. Moreover, unlike other approaches (discussed below), their method can be applied to virtually any model (involving i.i.d. data or not) and it does not require any assumption on the likelihood function nor on the prior distribution.

# Comparison with other approaches

The authors note that sub-sampling of the full dataset strategy has been proposed elsewhere, in particular, by @Korattikara2014, @Bardenet2014 and  @Maclaurin2015. The key difference between their approach and these is that the Markov chain transition kernel only uses a fraction $n/N$ of the available data which is by construction held constant throughout the algorithm. It is the subset variable that is randomly refreshed at each iteration according to the similarity measure.

Other similar approaches to solve these big data problems are @Quiroz2018 and the \textit{Confidence Sampler} in @Bardenet2017. Both of these approaches use sophisticated "control variates" to get positive unbiased estimators (based on a subset of data) for the likelihoods in the MRH acceptance ratio. The authors note that these control variates are still computationally intensive.

The authors also find some affinity to their work in \textit{"noisy"} or \textit{"inexact"} approaches to MCMC due to @Korattikara2014 and @Alquier2016, where an approximate MRH rule based on a sequential hypothesis test is used accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. The cost here is that the number of likelihood evaluations is adaptively set by the algorithm at each iteration. When the chain reaches equilibrium, the computational complexity is of order $O(n)$. This number can be brought down if an accurate proxy of the log-likelihood ratio, acting as control variates, is available, as demonstrated in @Bardenet2017.

Another approach which the authors compare their approach with is an approach based on continuous time Markov processes (Langevin diffusion, Zig-Zag process) in @Fearnhead2018 and @Bierkens2019. Here, the authors note that the computational hurdle involves calculation of the gradient of the log-likelihood, which may not always be unbiased. Moreover, these approaches depart significantly from the simplicity of the original discrete MRH algorithm.In the following sections we consider in more detail the main ideas of how this algorithm works (Section 3) and implement one of their examples of estimating the parameters in a logistic regression model with $N = 10^{6}$ using their algorithm.

# Main ideas of how it works

Let $(Y_{1}, \dots, Y_N)$ be a set of observed data and define
$Y_{U} = \{Y_{k}, k \in U \}$ where $U \subset \{1, \dots, N\}$. Let $\mathcal{S}: Y \to S\subseteq \mathbb{R}^{s}$. So $\mathcal{S}$ takes the data and returns an $s$ dimensional function of them, $S\subseteq \mathbb{R}^{s}$. If the model admits a sufficient statistic, then $\mathcal{S}$ maps $Y$ or a subset of it $Y_{U}$ to it, otherwise $\mathcal{S}$ maps $Y$ or a subset of it $Y_{U}$ to summary statistics $\bar{S} = S(Y_{U})/n$. See @Maire2019 p. 452, 453, 456. 

<!-- Summary statistics are a set of numerical measures that are used to describe the essential features of a data set. These measures provide a quick and easy way to understand the key characteristics of a dataset, such as its center, spread, and shape. The most commonly used summary statistics include measures of central tendency, such as the mean, median, and mode, which give an indication of where the data cluster around. Measures of variability, such as the standard deviation and range, provide information about how spread out the data are. Other summary statistics, such as skewness and kurtosis, give an indication of the shape of the distribution. By using summary statistics, researchers and data analysts can quickly get a sense of what the data looks like, which can help guide further analysis and interpretation. -->

<!-- The choice of the summary statistics in the algorithm is -->
<!-- problem specific and is meant to be the counterpart of the -->
<!-- sufficient statistic mapping for general models (hence sharing, slightly abusively, the same notation). Since the question -->
<!-- of specifying summary statistics also arises in Approximate Bayesian Computation (ABC), one can take advantage -->
<!-- of the abundant ABC literature on this topic to find some -->
<!-- examples of summary statistics for usual likelihood models. -->

For all $n \leq N$ , they define $\textsf{U}_{n}$ as the set of possible combinations of $n$ different integer numbers less than or equal to $N$ and $\mathcal{U}_{n}$ as the powerset of $\textsf{U}_{n}$. For any subset $U \in \textsf{U}_{n}$ define the vector of difference of sufficient statistics between the whole dataset and the subset $Y_{U}$ to be:

\begin{equation}\label{eqn1}
\Delta_n(U) = \sum_{k = 1}^{N}\mathcal{S}(Y_{k}) - N/n\sum_{k \in U}\mathcal{S}(Y_{k})
\end{equation}

If there is no sufficient statistic define an analogous difference measure using summary statistics as:

\begin{equation}\label{eqn2}
\bar{\Delta}_{n}(U) = S(Y)/N - S(Y_{U})/n
\end{equation}


## Similarity through summary statistics

They then consider the distribution $\nu_{n, \epsilon}$ on the discrete space $\textsf{U}_{n}$ defined for all $\epsilon \geq 0$ by:

\begin{equation}\label{eqn3}
\nu_{n, \epsilon}(U) \propto \text{exp}(-\epsilon ||\Delta_{n}(U) ||^2)
\end{equation}

The distribution $\nu_{n, \epsilon}$ assigns a weight to any subset according to its representativeness with respect to the full dataset. It is a kind of tuning parameter. When $\epsilon = 0$, $\nu_{n, \epsilon}$ is uniform on $U_{n}$, while when $\epsilon \to \infty$, $\nu_{n, \epsilon}$ is uniform on the set of subsets that minimize $||\Delta_{n}(U)||$. The authors note that moving to general models (i.e. non i.i.d and non-exponential) amounts to relaxing the sufficient statistics existence assumption as well as the $\epsilon \to \infty$ condition. This is achieved by constructing a class of summary statistics for the model at hand and for which the following result holds. (We do not go into the details here - See @Maire2019 p. 454 Proposition 3)

For any $\theta \in \Theta$ and $\epsilon \> 0$, there exists $M < \infty$ such that 
\[
\mathbb{E}_{n, \epsilon} \bigg\{\frac{f(Y | \theta)}{f(Y_{U}|\theta)^{N/n}}\bigg\} < M
\]

## Setting up the algorithm

[Jizhi add details here Section 4.2.2]

In the Informed Sub-Sampling method, the set of good subsamples is treated as a series of missing data (denoted by $U_{1}$, $U_{2}$, and so on) and is simulated using the ISS-MCMC algorithm. This algorithm generates a Markov chain on the extended space 
$\theta \times  \mathcal{U}_{n}$, where $\mathcal{U}_{n}$ is the set of possible subsamples. The sequence of subsamples is randomly updated in a way that favors those subsets with summary statistics that are similar to the full dataset, as suggested by the analysis in previous sections. The process is implemented using a symmetric transition kernel R on $(\mathcal{U}_n, U_n)$. A transition from $(\theta_i,U_i)$ to $(\theta_{I+1} , U_{i+1})$ involves two steps:

(i)

(a) Propose a new subset variable $U \sim R(U_i, \cdot)$

(b) Set $U_{i+1}=U$ with probability 
\begin{equation}
\beta(U_i, U)=1 \wedge b(U_i, U)
\end{equation}
, where 
\begin{equation}
b(U_i, U) = \exp \left( \epsilon(|\Delta_n(U_i)|^2 - |\Delta_n(U)|^2) \right)
\end{equation}, and $U_{i+1} = U_i$ with probability $1 - \beta(U_i, U)$. Here, $\Delta_n$ is defined in equation 2. The transition kernel R is selected based on the data and distribution.

(ii)

(a)propose a new parameter $\theta \sim Q(\theta_i, \cdot)$

(b)Set $\theta_{i+1}=\theta$ with probability 
\begin{equation}
\alpha(\theta_i, \theta)=1 \wedge a(\theta_i, \theta)
\end{equation}
, where 
\begin{equation}
a(\theta_i, \theta) =  \frac{\pi_n(\theta \mid Y_{U_{i+1}})Q(\theta, \theta_i)} {\pi_n(\theta_i \mid Y_{U_{i+1}})Q(\theta_i, \theta)}
\end{equation}, and $\theta_{i+1} = \theta_i$ with probability $1 - \alpha(U_i, U)$. The transition kernel Q is selected based on the data and distribution.

## The Algorithm

[David to type out the table here]

# Example with logistic regression

Here we implement one example from their paper. The example involves simulated logistic regression data with $N= 10^6$ observations and $n = 1000, 5000$ and $1000$

Consider the logistic regression model:
\begin{equation}
P(Y=1) = \frac{\exp(\beta_0 + \beta_1X_1 + \beta_2X_2)}{1+\exp(\beta_0 + \beta_1X_1 + \beta_2X_2)}
\end{equation},


[Jizhi and David section 6.3]

```{r "MCMC code", echo=FALSE}
##add our code here.

```


Here are marginal distributions and traceplots obtained using the ISS-MCMC algorithm.

```{r "plots", echo=FALSE}


```


## Compare computational time of ISS-MCMC with M-H.

Finally, we present tables that compare the computational time of using ISS-MCMC,

```{r "tab1", echo=FALSE}
#add tables here using system.time() function.

```


# Conclusion and Future Work

[David add details here]

# References




